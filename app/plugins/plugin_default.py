import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew, kurtosis
import json

class Plugin:
    """
    Plugin to preprocess the dataset for feature extraction.
    This version generates hourly and daily predictions with header names
    compatible with the heuristic system, and retains only DATE_TIME and prediction columns.
    """
    # Define the parameters for this plugin and their default values
    plugin_params = {
        'target_column': 'CLOSE',
        'time_horizon': 6,        # Number of hourly predictions
        'ticks_per_day': 24,      # Number of ticks per day (assume hourly data by default)
        'days_horizon': 6,        # Number of daily predictions
        'std_dev_horizon': 12     # Optional: for rolling standard deviation calculation (not used now)
    }

    # Define the debug variables for this plugin
    plugin_debug_vars = ['column_metrics', 'normalization_params']

    def __init__(self):
        """
        Initialize the Plugin with default parameters.
        """
        self.params = self.plugin_params.copy()
        self.normalization_params = {}  # To store normalization parameters for each column
        self.column_metrics = {}        # To store metrics for each column

    def set_params(self, **kwargs):
        """
        Set the parameters for the plugin.

        Args:
            **kwargs: Arbitrary keyword arguments for plugin parameters.
        """
        for key, value in kwargs.items():
            if key in self.params:
                self.params[key] = value

    def get_debug_info(self):
        """
        Get debug information for the plugin.

        Returns:
            dict: Debug information including column metrics and normalization parameters.
        """
        debug_info = {
            'column_metrics': self.column_metrics,
            'normalization_params': self.normalization_params
        }
        return debug_info

    def add_debug_info(self, debug_info):
        """
        Add debug information to the given dictionary.

        Args:
            debug_info (dict): The dictionary to add debug information to.
        """
        debug_info.update(self.get_debug_info())

    def process(self, data):
        """
        Generate a dataset with hourly and daily predictions.
        
        The output dataframe will have the following columns:
          - DATE_TIME
          - Prediction_h_1, ..., Prediction_h_{time_horizon}
          - Prediction_d_1, ..., Prediction_d_{days_horizon}

        The predictions are generated by shifting the target column.
        Unwanted columns (OPEN, HIGH, LOW, CLOSE, std_dev) are removed.
        """
        print(f"[DEBUG] Loaded data shape: {data.shape}")
        print(f"[DEBUG] Columns in the data: {list(data.columns)}")

        # Step 1: Ensure DATE_TIME column is included as a regular column
        if isinstance(data.index, pd.DatetimeIndex):
            print("[DEBUG] DATE_TIME is currently the index. Resetting it to a regular column...")
            data.reset_index(inplace=True)
        if 'DATE_TIME' not in data.columns:
            raise ValueError("[ERROR] DATE_TIME column is missing in the input data!")

        target_column = self.params['target_column']
        print(f"[DEBUG] Target column: {target_column}")
        if target_column not in data.columns:
            raise ValueError(f"[ERROR] Target column '{target_column}' is missing in the input data!")

        # Step 2: Extract only DATE_TIME and target column for processing
        columns_to_extract = ['DATE_TIME', target_column]
        processed_data = data[columns_to_extract].copy()
        print(f"[DEBUG] Extracted columns: {list(processed_data.columns)}")

        # Step 3: Generate hourly predictions from the target column.
        time_horizon = self.params['time_horizon']
        for i in range(1, time_horizon + 1):
            processed_data[f'Prediction_h_{i}'] = processed_data[target_column].shift(-i)

        # Step 4: Generate daily predictions
        ticks_per_day = self.params['ticks_per_day']
        days_horizon = self.params['days_horizon']
        for i in range(1, days_horizon + 1):
            processed_data[f'Prediction_d_{i}'] = processed_data[target_column].shift(-i * ticks_per_day)

        # Step 5: Drop rows with NaN values resulting from shifts
        initial_shape = processed_data.shape
        processed_data.dropna(inplace=True)
        final_shape = processed_data.shape
        print(f"[DEBUG] Processed data shape before dropping NaN: {initial_shape}")
        print(f"[DEBUG] Processed data shape after dropping NaN: {final_shape}")

        # Step 6: Remove the original target column (and any other non-prediction columns)
        if target_column in processed_data.columns:
            processed_data.drop(columns=[target_column], inplace=True)

        # Step 7: Reset index and ensure chronological order
        processed_data.reset_index(drop=True, inplace=True)
        processed_data.sort_values(by='DATE_TIME', inplace=True)
        processed_data.reset_index(drop=True, inplace=True)

        print(f"[DEBUG] Final processed data shape: {processed_data.shape}")
        return processed_data


# Example usage
if __name__ == "__main__":
    plugin = Plugin()
    data = pd.read_csv('tests/data/EURUSD_5m_2010_2015.csv', header=None)
    # If necessary, assign appropriate column names. For example:
    # data.columns = ['DATE_TIME', 'OPEN', 'HIGH', 'LOW', 'CLOSE', ...]
    print(f"Loaded data shape: {data.shape}")
    processed_data = plugin.process(data)
    print(processed_data.head())
